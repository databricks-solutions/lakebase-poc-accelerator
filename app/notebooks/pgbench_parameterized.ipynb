{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parameterized pgbench Test for Databricks Jobs\n",
        "This notebook runs pgbench tests with configurable parameters for use in Databricks Jobs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Setup Requirements\n",
        "%pip install --upgrade databricks-sdk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dbutils.library.restartPython()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install pgbench\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%sh\n",
        "apt-get update && apt-get install -y wget gnupg lsb-release\n",
        "sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" \\\n",
        "    > /etc/apt/sources.list.d/pgdg.list'\n",
        "wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | apt-key add -\n",
        "apt-get update\n",
        "apt-get install -y postgresql-client-15\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%sh\n",
        "apt-get install -y postgresql-contrib-15\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%sh\n",
        "pgbench --version\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parse Job Parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, subprocess, re, glob, numpy as np, json\n",
        "from databricks.sdk import WorkspaceClient\n",
        "import uuid, shutil as _shutil\n",
        "\n",
        "# Parse job parameters with defaults\n",
        "def get_param(param_name, default_value, param_type=str):\n",
        "    \"\"\"Get parameter from dbutils.widgets or use default\"\"\"\n",
        "    try:\n",
        "        value = dbutils.widgets.get(param_name)\n",
        "        if value:\n",
        "            if param_type == int:\n",
        "                return int(value)\n",
        "            elif param_type == float:\n",
        "                return float(value)\n",
        "            elif param_type == bool:\n",
        "                return value.lower() in ['true', '1', 'yes']\n",
        "            else:\n",
        "                return value\n",
        "    except:\n",
        "        pass\n",
        "    return default_value\n",
        "\n",
        "# Job parameters\n",
        "LAKEBASE_INSTANCE_NAME = get_param(\"lakebase_instance_name\", \"ak-lakebase-accelerator-instance\")\n",
        "DATABASE_NAME = get_param(\"database_name\", \"databricks_postgres\")\n",
        "PGBENCH_CLIENTS = get_param(\"pgbench_clients\", 8, int)\n",
        "PGBENCH_JOBS = get_param(\"pgbench_jobs\", 8, int)\n",
        "PGBENCH_DURATION = get_param(\"pgbench_duration\", 30, int)\n",
        "PGBENCH_PROGRESS_INTERVAL = get_param(\"pgbench_progress_interval\", 5, int)\n",
        "PGBENCH_PROTOCOL = get_param(\"pgbench_protocol\", \"prepared\")\n",
        "PGBENCH_PER_STATEMENT_LATENCY = get_param(\"pgbench_per_statement_latency\", True, bool)\n",
        "PGBENCH_DETAILED_LOGGING = get_param(\"pgbench_detailed_logging\", True, bool)\n",
        "PGBENCH_CONNECT_PER_TRANSACTION = get_param(\"pgbench_connect_per_transaction\", False, bool)\n",
        "\n",
        "# Query configuration (JSON string)\n",
        "QUERY_CONFIG_JSON = get_param(\"query_config\", '[]')\n",
        "\n",
        "print(f\"Parameters:\")\n",
        "print(f\"  Lakebase Instance: {LAKEBASE_INSTANCE_NAME}\")\n",
        "print(f\"  Database: {DATABASE_NAME}\")\n",
        "print(f\"  Clients: {PGBENCH_CLIENTS}\")\n",
        "print(f\"  Jobs: {PGBENCH_JOBS}\")\n",
        "print(f\"  Duration: {PGBENCH_DURATION}s\")\n",
        "print(f\"  Protocol: {PGBENCH_PROTOCOL}\")\n",
        "print(f\"  Query Config: {QUERY_CONFIG_JSON}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Connection and Queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# 1) Connection env\n",
        "# -------------------------\n",
        "w = WorkspaceClient()\n",
        "instance = w.database.get_database_instance(name=LAKEBASE_INSTANCE_NAME)\n",
        "cred = w.database.generate_database_credential(request_id=str(uuid.uuid4()),\n",
        "                                               instance_names=[LAKEBASE_INSTANCE_NAME])\n",
        "\n",
        "env = os.environ.copy()\n",
        "env.update({\n",
        "    \"PGHOST\": instance.read_write_dns,\n",
        "    \"PGPORT\": \"5432\",\n",
        "    \"PGDATABASE\": DATABASE_NAME,\n",
        "    \"PGUSER\": w.current_user.me().user_name,\n",
        "    \"PGPASSWORD\": cred.token,\n",
        "    \"PGSSLMODE\": \"require\",\n",
        "})\n",
        "\n",
        "print(\"pgbench at:\", _shutil.which(\"pgbench\"))\n",
        "\n",
        "# -------------------------\n",
        "# 2) Parse query configuration and write scripts locally\n",
        "# -------------------------\n",
        "workdir = \"/databricks/driver/pgbench_mix\"\n",
        "os.makedirs(workdir, exist_ok=True)\n",
        "\n",
        "try:\n",
        "    query_configs = json.loads(QUERY_CONFIG_JSON)\n",
        "except json.JSONDecodeError:\n",
        "    print(\"Invalid query config JSON, using default queries\")\n",
        "    query_configs = []\n",
        "\n",
        "# Default queries if none provided\n",
        "if not query_configs:\n",
        "    query_configs = [\n",
        "        {\n",
        "            \"name\": \"point\",\n",
        "            \"content\": \"\\\\set c_customer_sk random(0, 999)\\nSELECT *\\nFROM databricks_postgres.public.customer\\nWHERE c_customer_sk = :c_customer_sk;\",\n",
        "            \"weight\": 60\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"range\",\n",
        "            \"content\": \"\\\\set c_current_hdemo_sk random(1, 700)\\nSELECT count(*)\\nFROM databricks_postgres.public.customer\\nWHERE c_current_hdemo_sk BETWEEN :c_current_hdemo_sk AND :c_current_hdemo_sk + 1000;\",\n",
        "            \"weight\": 30\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"agg\",\n",
        "            \"content\": \"SELECT c_preferred_cust_flag, count(*)\\nFROM databricks_postgres.public.customer\\nGROUP BY c_preferred_cust_flag;\",\n",
        "            \"weight\": 10\n",
        "        }\n",
        "    ]\n",
        "\n",
        "# Write query files\n",
        "query_files = []\n",
        "for query_config in query_configs:\n",
        "    query_name = query_config.get(\"name\", \"query\")\n",
        "    query_content = query_config.get(\"content\", \"\")\n",
        "    \n",
        "    query_path = os.path.join(workdir, f\"{query_name}.sql\")\n",
        "    with open(query_path, \"w\") as f:\n",
        "        f.write(query_content.strip() + \"\\n\")\n",
        "    \n",
        "    query_files.append((query_path, query_config.get(\"weight\", 1)))\n",
        "    print(f\"Created query file: {query_path}\")\n",
        "\n",
        "# Verify all files exist\n",
        "for query_path, _ in query_files:\n",
        "    assert os.path.exists(query_path), f\"Missing script: {query_path}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Execute pgbench Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# 3) Build pgbench command\n",
        "# -------------------------\n",
        "cmd = [\n",
        "    \"pgbench\",\n",
        "    \"-n\",  # no vacuuming\n",
        "    \"-c\", str(PGBENCH_CLIENTS),\n",
        "    \"-j\", str(PGBENCH_JOBS),\n",
        "    \"-T\", str(PGBENCH_DURATION),\n",
        "    \"-P\", str(PGBENCH_PROGRESS_INTERVAL),\n",
        "    \"-M\", PGBENCH_PROTOCOL,\n",
        "]\n",
        "\n",
        "# Add optional flags\n",
        "if PGBENCH_PER_STATEMENT_LATENCY:\n",
        "    cmd.append(\"-r\")\n",
        "\n",
        "if PGBENCH_DETAILED_LOGGING:\n",
        "    cmd.append(\"-l\")\n",
        "\n",
        "if PGBENCH_CONNECT_PER_TRANSACTION:\n",
        "    cmd.append(\"-C\")\n",
        "\n",
        "# Add query files with weights (simulate weights by repeating -f)\n",
        "for query_path, weight in query_files:\n",
        "    for _ in range(int(weight)):\n",
        "        cmd.extend([\"-f\", query_path])\n",
        "\n",
        "print(f\"pgbench command: {' '.join(cmd)}\")\n",
        "\n",
        "# -------------------------\n",
        "# 4) Run & parse output\n",
        "# -------------------------\n",
        "print(\"\\n=== Starting pgbench test ===\")\n",
        "res = subprocess.run(cmd, capture_output=True, text=True, env=env, cwd=workdir)\n",
        "\n",
        "print(\"=== STDOUT ===\\n\", res.stdout)\n",
        "if res.stderr:\n",
        "    print(\"=== STDERR ===\\n\", res.stderr)\n",
        "\n",
        "if res.returncode != 0:\n",
        "    raise SystemExit(f\"pgbench failed (exit {res.returncode}). See STDERR above. Workdir: {workdir}\")\n",
        "\n",
        "# Parse TPS\n",
        "m = re.search(r\"tps\\s*=\\s*([\\d\\.]+)\", res.stdout)\n",
        "tps = float(m.group(1)) if m else None\n",
        "print(f\"\\n=== RESULTS ===\")\n",
        "print(f\"TPS: {tps}\")\n",
        "\n",
        "# Parse latencies from log files\n",
        "latencies = []\n",
        "for path in glob.glob(os.path.join(workdir, \"pgbench_log.*\")):\n",
        "    with open(path) as f:\n",
        "        for line in f:\n",
        "            parts = line.split()\n",
        "            if parts:\n",
        "                try:\n",
        "                    latencies.append(float(parts[-1]))  # last col = latency ms\n",
        "                except ValueError:\n",
        "                    pass\n",
        "\n",
        "if latencies:\n",
        "    p50, p95, p99 = np.percentile(latencies, [50, 95, 99])\n",
        "    print(f\"Latency p50/p95/p99 (ms): {p50:.3f} / {p95:.3f} / {p99:.3f}\")\n",
        "    print(f\"Total transactions: {len(latencies)}\")\n",
        "else:\n",
        "    print(\"No pgbench_log.* found or no latencies parsed.\")\n",
        "\n",
        "print(f\"\\nLogs & scripts available at: {workdir}\")\n",
        "\n",
        "# -------------------------\n",
        "# 5) Store results for job output\n",
        "# -------------------------\n",
        "results = {\n",
        "    \"test_parameters\": {\n",
        "        \"lakebase_instance\": LAKEBASE_INSTANCE_NAME,\n",
        "        \"database_name\": DATABASE_NAME,\n",
        "        \"clients\": PGBENCH_CLIENTS,\n",
        "        \"jobs\": PGBENCH_JOBS,\n",
        "        \"duration_seconds\": PGBENCH_DURATION,\n",
        "        \"protocol\": PGBENCH_PROTOCOL,\n",
        "        \"query_count\": len(query_configs)\n",
        "    },\n",
        "    \"performance_metrics\": {\n",
        "        \"tps\": tps,\n",
        "        \"total_transactions\": len(latencies),\n",
        "        \"latency_p50_ms\": p50 if latencies else None,\n",
        "        \"latency_p95_ms\": p95 if latencies else None,\n",
        "        \"latency_p99_ms\": p99 if latencies else None\n",
        "    },\n",
        "    \"test_status\": \"completed\" if res.returncode == 0 else \"failed\",\n",
        "    \"raw_output\": res.stdout\n",
        "}\n",
        "\n",
        "# Save results to a file that can be accessed by the job\n",
        "results_path = os.path.join(workdir, \"pgbench_results.json\")\n",
        "with open(results_path, \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"\\n=== Test completed successfully! ===\")\n",
        "print(f\"Results saved to: {results_path}\")\n",
        "print(json.dumps(results, indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
